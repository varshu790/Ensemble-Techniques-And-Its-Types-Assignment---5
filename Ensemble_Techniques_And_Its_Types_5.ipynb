{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature\n",
        "engineering process and handles the missing values.\n",
        "\n",
        "Design a pipeline that includes the following steps:\n",
        "\n",
        " - Use an automated feature selection method to identify the important features in the dataset\n",
        "\n",
        " - Create a numerical pipeline that includes the following steps:\n",
        "\n",
        " - Impute the missing values in the numerical columns using the mean of the column values\n",
        "\n",
        " - Scale the numerical columns using standardisation\n",
        " - Create a categorical pipeline that includes the following steps:\n",
        "\n",
        " - Impute the missing values in the categorical columns using the most frequent value of the column\n",
        "\n",
        " - One-hot encode the categorical columns\n",
        "\n",
        " - Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
        "\n",
        " - Use a Random Forest Classifier to build the final model\n",
        "\n",
        " - Evaluate the accuracy of the model on the test dataset\n",
        "\n",
        "Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline."
      ],
      "metadata": {
        "id": "nPq8AnTEPKv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generating numerical features in the range [0, 4]\n",
        "numerical_features = np.random.uniform(0, 4, size=(1000, 5))\n",
        "\n",
        "# Generating categorical features in the range [-5, 0]\n",
        "categorical_features = np.random.randint(-5, 1, size=(1000, 5))\n",
        "\n",
        "# Concatenating numerical and categorical features\n",
        "X = np.concatenate((numerical_features, categorical_features), axis=1)\n",
        "\n",
        "# Generating a random target variable\n",
        "y = np.random.randint(0, 2, size=1000)\n",
        "\n",
        "# Assuming numerical and categorical indices are [0, 1, 2, 3, 4] and [5, 6, 7, 8, 9] respectively\n",
        "numerical_indices = [0, 1, 2, 3, 4]  # indices for numerical features\n",
        "categorical_indices = [5, 6, 7, 8, 9]  # indices for categorical features\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Numerical pipeline for numerical features\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline for categorical features\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combining numerical and categorical pipelines using ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numerical_pipeline, numerical_indices),\n",
        "    ('cat', categorical_pipeline, categorical_indices)\n",
        "])\n",
        "\n",
        "# Feature selection using SelectFromModel with RandomForestClassifier\n",
        "feature_selection = SelectFromModel(RandomForestClassifier(random_state=42))\n",
        "\n",
        "# Creating the final pipeline with feature selection, preprocessing, and RandomForestClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('feature_selection', feature_selection),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Fitting the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the model: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow56o-GdU2SS",
        "outputId": "67a60470-c5c6-485b-863b-499cb16b3aca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation of Steps:**\n",
        "\n",
        "1. **Data Generation**: Simulated the numerical and categorical features along with the target variable.\n",
        "2. **Splitting Data**: Divided the data into training and testing sets.\n",
        "3. **Numerical Pipeline**: Created a pipeline to handle numerical features by imputing missing values with the mean and scaling using StandardScaler.\n",
        "4. **Categorical Pipeline**: Developed a pipeline for categorical features by imputing missing values with the most frequent value and performing one-hot encoding.\n",
        "5. **ColumnTransformer**: Combined both pipelines to process numerical and categorical features separately.\n",
        "6. **Feature Selection**: Utilized SelectFromModel to perform feature selection based on importance from a RandomForestClassifier.\n",
        "7. **Final Pipeline**: Constructed the final pipeline by combining preprocessing, feature selection, and RandomForestClassifier.\n",
        "8. **Model Training and Evaluation**: Fitted the pipeline on the training data, made predictions on the test set, and evaluated the model's accuracy.\n",
        "\n",
        "**Interpretation & Possible Improvements:**\n",
        "\n",
        "- The pipeline automates preprocessing, feature selection, and model building, providing a convenient workflow.\n",
        "- Interpret the feature importance obtained from SelectFromModel to understand the most influential features.\n",
        "- Experiment with different hyperparameters and models for better performance.\n",
        "- Perform cross-validation to assess the model's generalization.\n",
        "- Incorporate more advanced techniques like hyperparameter tuning or ensemble methods for enhancing model accuracy."
      ],
      "metadata": {
        "id": "JXMQGO_PVEC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
        "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "Ee1k9pB7VTbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating pipelines for individual classifiers\n",
        "rf_pipeline = Pipeline([\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "lr_pipeline = Pipeline([\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# Creating a Voting Classifier combining both individual classifiers\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[('rf', rf_pipeline), ('lr', lr_pipeline)],\n",
        "    voting='hard'  # 'hard' voting means majority voting\n",
        ")\n",
        "\n",
        "# Training the Voting Classifier\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test data\n",
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Voting Classifier: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7_uiuOpV1jC",
        "outputId": "cf8dd5e9-4cfe-4bc3-ac62-a021874c20c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Voting Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's an example of building a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then combines their predictions using a Voting Classifier. This pipeline will be trained on the Iris dataset and evaluated for accuracy:\n",
        "\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. **Loading Data**: Loaded the Iris dataset using `load_iris()` from scikit-learn.\n",
        "2. **Splitting Data**: Split the dataset into training and testing sets.\n",
        "3. **Individual Pipelines**: Created separate pipelines for Random Forest Classifier and Logistic Regression Classifier.\n",
        "4. **Voting Classifier**: Created a Voting Classifier that combines the predictions of the individual classifiers.\n",
        "5. **Training**: Fit the Voting Classifier on the training data.\n",
        "6. **Prediction and Evaluation**: Made predictions on the test set and evaluated the accuracy of the Voting Classifier.\n",
        "\n",
        "This Voting Classifier combines the predictions of Random Forest and Logistic Regression, making a final decision based on majority voting. Adjust the hyperparameters and voting method ('soft' or 'hard') based on the dataset and desired ensemble strategy."
      ],
      "metadata": {
        "id": "_Txl397UV7PE"
      }
    }
  ]
}